{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "a-Rys0s51hxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import trange, tqdm\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from xai_transformer import BertAttention, GammaParams, LNargsDetach, GammaNet, GammaParams\n",
        "from utils import LayerNorm, flip"
      ],
      "metadata": {
        "id": "-ZRWY5lWz8xG"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration"
      ],
      "metadata": {
        "id": "ZgRH8O4X1jUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m22o5qYdz-Ye",
        "outputId": "c71ed03b-906d-4e51-a6ef-6022286ce2f8"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMep13tqz_gZ",
        "outputId": "30451fed-61a8-41bc-e98c-7cec2a5d6764"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer_config.json\n",
            "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/tokenizer.json\n",
            "loading file chat_template.jinja from cache at None\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConfig:\n",
        "    hidden_size = bert_config.hidden_size\n",
        "    all_head_size = bert_config.hidden_size\n",
        "    num_attention_heads = bert_config.num_attention_heads\n",
        "    attention_head_size = bert_config.hidden_size // bert_config.num_attention_heads\n",
        "    n_blocks = 4\n",
        "    n_classes = 2\n",
        "    device = device\n",
        "    train_mode = False\n",
        "    detach_layernorm = True\n",
        "    detach_mean = True\n",
        "    detach_kq = True\n",
        "    layer_norm_eps = 1e-5\n",
        "    gamma_LN = 0.0\n",
        "    gamma_AH = 0.0\n",
        "\n",
        "config = SimpleConfig()"
      ],
      "metadata": {
        "id": "Mb45b1am0BJ8"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing models"
      ],
      "metadata": {
        "id": "84ZtrGwN05FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "hf_embeddings = hf_bert.embeddings\n",
        "\n",
        "# Replacing layer norm with LayerNormImpl\n",
        "hf_embeddings = hf_embeddings  # keep same object\n",
        "hf_embeddings.LayerNorm = LayerNorm(bert_config.hidden_size, eps = bert_config.layer_norm_eps,\n",
        "                                   args = LNargsDetach())\n",
        "\n",
        "hf_embeddings = hf_embeddings.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMLvBjD90EoA",
        "outputId": "dc1ebd69-2dad-4301-f627-84fdcaad2ac2"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.57.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/86b5e0934494bd15c9632b12f734a8a67f723594/model.safetensors\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertAttention(config, hf_embeddings).to(device)\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EunIJcxe0GMP",
        "outputId": "a75c713c-92f9-4b7c-c73d-b1bc8f2312ce"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detach LayerNorm Mean+Norm\n",
            "Detach K-Q-softmax branch\n",
            "Detach LayerNorm Mean+Norm\n",
            "Detach K-Q-softmax branch\n",
            "Detach LayerNorm Mean+Norm\n",
            "Detach K-Q-softmax branch\n",
            "Detach LayerNorm Mean+Norm\n",
            "Detach K-Q-softmax branch\n",
            "Detach LayerNorm Mean+Norm\n",
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_dim = hf_embeddings.word_embeddings.weight.shape[1]\n",
        "gamma_net = GammaNet(hidden_dim=64, n_blocks=config.n_blocks, pooled_dim=pooled_dim).to(device)\n",
        "print(\"GammaNet created:\", gamma_net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0haOc6F50ICp",
        "outputId": "1a6ae795-c212-48b8-d38c-56fcefe03029"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GammaNet created: GammaNet(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=8, bias=True)\n",
            "    (3): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"GammaNet has a total of {sum(p.numel() for p in gamma_net.parameters())} trainable parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgWzfT-z0K18",
        "outputId": "14a88a01-a27f-42b5-f423-a4ba51b42937"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GammaNet has a total of 49736 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma_net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3urhqCWF5fYb",
        "outputId": "0dd9d5c3-e5c8-4ae9-b6d1-d6bd8afaf548"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GammaNet(\n",
              "  (net): Sequential(\n",
              "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=64, out_features=8, bias=True)\n",
              "    (3): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing data"
      ],
      "metadata": {
        "id": "TidcZSDa07M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"glue\", \"sst2\")\n",
        "val_ds = ds[\"validation\"]\n",
        "print(\"Validation size:\", len(val_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik7HP34S0w1t",
        "outputId": "713bd038-6363-40d4-bd9c-6236661205c2"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation size: 872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SAMPLES = 200\n",
        "VAL_SAMPLES = 100"
      ],
      "metadata": {
        "id": "r3rK8DEc00fo"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_iter(dataset, max_samples=None):\n",
        "    cnt = 0\n",
        "    for ex in dataset:\n",
        "        enc = tokenizer(ex[\"sentence\"], truncation=True, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n",
        "        input_ids = enc[\"input_ids\"].long()  # shape [1, seq_len]\n",
        "        att_mask = enc[\"attention_mask\"].long()\n",
        "        label = int(ex[\"label\"])\n",
        "        yield input_ids.to(device), att_mask.to(device), label\n",
        "        cnt += 1\n",
        "        if max_samples is not None and cnt >= max_samples:\n",
        "            break"
      ],
      "metadata": {
        "id": "etwG-Rf10zID"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter = list(dataset_iter(val_ds, max_samples=TRAIN_SAMPLES))\n",
        "val_iter = list(dataset_iter(val_ds, max_samples=VAL_SAMPLES))\n",
        "print(\"Prepared train/val subsets:\", len(train_iter), len(val_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKA-aSmF02I8",
        "outputId": "46e1f593-893c-4a94-c6f8-bd97c00ea0da"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared train/val subsets: 200 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Hyperparameters and Helper functions"
      ],
      "metadata": {
        "id": "iTGoLxXB09l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "lr = 1e-5\n",
        "comp_weight = 10.0 # weight on completeness loss\n",
        "auac_weight = 1.0 # weight on the AUAC proxy\n",
        "gamma_reg = 1e-2\n",
        "topk_frac = 0.1 # for AUAC proxy\n",
        "exact_every = 200 # run exact flip eval every _ steps\n",
        "do_exact_eval = True\n",
        "save_path = \"/mnt/data/gamma_net_results.pkl\""
      ],
      "metadata": {
        "id": "NNgTO2_W1Bah"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pooled_embedding_from_embeds(input_ids):\n",
        "    # produce embeddings and pooled CLS embedding\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            embeds_out = model.embeds(input_ids=input_ids)\n",
        "        except TypeError:\n",
        "            embeds_out = model.embeds(input_ids)\n",
        "    pooled = embeds_out[:, 0, :].to(device)   # [1, hidden]\n",
        "    return pooled"
      ],
      "metadata": {
        "id": "1t5IPtW71NOf"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_topk_by_R(input_ids, R_tensor, frac, pad_id=0):\n",
        "    # R_tensor expected torch.Tensor or numpy-like vector; handle both\n",
        "    if isinstance(R_tensor, torch.Tensor):\n",
        "        R_np = R_tensor.detach().cpu().numpy().flatten()\n",
        "    else:\n",
        "        R_np = np.array(R_tensor).flatten()\n",
        "    seq_len = input_ids.shape[1]\n",
        "    k = max(1, int(seq_len * frac))\n",
        "    topk_idx = np.argsort(-R_np)[:k]\n",
        "    masked = input_ids.clone()\n",
        "    masked[0, topk_idx] = pad_id\n",
        "    return masked"
      ],
      "metadata": {
        "id": "I5aoEj251QJ_"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = {\"step\": [], \"loss\": [], \"comp\": [], \"gamma_mean\": [], \"val_comp\": [], \"val_auac\": [], \"val_aumse\": [],\n",
        "           \"gamma_AH\": [], \"gamma_LN\": [], \"raw_logit\": [], \"sumR\": []}\n",
        "\n",
        "step = 0"
      ],
      "metadata": {
        "id": "7g0nsQ3L1TDU"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze Bert Weights\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# unfreeze GammaNet weights\n",
        "for p in gamma_net.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "optimizer = optim.Adam(gamma_net.parameters(), lr=lr, weight_decay = 1e-4)"
      ],
      "metadata": {
        "id": "P3qDbWQamLfq"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0"
      ],
      "metadata": {
        "id": "7_FxGtojsjck"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training loop...\")\n",
        "for epoch in range(epochs):\n",
        "    random.shuffle(train_iter)\n",
        "    for input_ids, att_mask, label in tqdm(train_iter, desc = f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        step += 1\n",
        "\n",
        "        # DIFFERENTIABLE FORWARD\n",
        "        res = model.forward_and_explain_differentiable(input_ids=input_ids, cl=label, gamma_net=gamma_net)\n",
        "\n",
        "        logits = res[\"logits\"]\n",
        "        R_out = res[\"R\"]\n",
        "        gamma_AH = res[\"gamma_AH\"]\n",
        "        gamma_LN = res[\"gamma_LN\"]\n",
        "\n",
        "        if torch.isnan(logits).any():\n",
        "            print(\"NAN in logits\")\n",
        "        if torch.isnan(R_out).any():\n",
        "            print(\"NAN in R_out\")\n",
        "        if torch.isnan(gamma_AH).any():\n",
        "            print(\"NAN in gamma_AH\")\n",
        "        if torch.isnan(gamma_LN).any():\n",
        "            print(\"NAN in gamma_LN\")\n",
        "\n",
        "        # LOSSES\n",
        "        target_logit = logits[0, label]\n",
        "        sumR = R_out.sum()\n",
        "        comp_loss = (target_logit - sumR).pow(2).mean()\n",
        "        gamma_reg_term = (gamma_AH.mean() + gamma_LN.mean()) * 0.5\n",
        "\n",
        "        # AUAC PROXY (NO GRAD)\n",
        "        # === AUAC PROXY (NO GRAD) ===\n",
        "        # === AUAC PROXY (NO GRAD) ===\n",
        "        with torch.no_grad():\n",
        "            masked_ids = mask_topk_by_R(input_ids, R_out, topk_frac, pad_id=tokenizer.pad_token_id)\n",
        "\n",
        "            # Use a simple forward pass without explanation\n",
        "            # First, add this simple forward method to your BertAttention class if not already there:\n",
        "            masked_out = model.forward_simple(masked_ids)  # We'll create this method\n",
        "            new_logit = float(masked_out['logits'][0, label].cpu().numpy())\n",
        "            auac_proxy = float(target_logit.detach().cpu().numpy()) - new_logit\n",
        "\n",
        "        # BACKPROP\n",
        "        loss = comp_weight * comp_loss + gamma_reg * gamma_reg_term\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Debug: Check if gradients are flowing to GammaNet\n",
        "        # print(\"\\n[GRAD CHECK] Before clipping:\")\n",
        "        # for name, param in gamma_net.named_parameters():\n",
        "        #     if param.grad is not None:\n",
        "        #         print(f\"  {name}: grad_norm = {param.grad.norm().item():.8f}\")\n",
        "        #     else:\n",
        "        #         print(f\"  {name}: NO GRADIENTS!\")\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(gamma_net.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # LOGGING\n",
        "        gmean = float(gamma_reg_term.detach().cpu().numpy())\n",
        "        history[\"step\"].append(step)\n",
        "        history[\"loss\"].append(float(loss.detach().cpu().numpy()))\n",
        "        history[\"comp\"].append(float(comp_loss.detach().cpu().numpy()))\n",
        "        history[\"gamma_mean\"].append(gmean)\n",
        "        history[\"gamma_AH\"].append(gamma_AH.detach().cpu().numpy())\n",
        "        history[\"gamma_LN\"].append(gamma_LN.detach().cpu().numpy())\n",
        "        history[\"raw_logit\"].append(float(target_logit.detach().cpu().numpy()))\n",
        "        history[\"sumR\"].append(float(sumR.detach().cpu().numpy()))\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(f\"step {step:5d} loss={loss.item():.4f} comp={comp_loss.item():.6f} auac_proxy={auac_proxy:.4f} gamma_mean={gmean:.4f}\")\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed; steps so far: {step}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYi7bvb-voVz",
        "outputId": "c07327ff-d872-4be3-f58b-7b334fedea4a"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training loop...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  10%|█         | 20/200 [00:08<01:05,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    20 loss=0.0975 comp=0.009269 auac_proxy=0.0740 gamma_mean=0.4818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  20%|██        | 40/200 [00:16<01:18,  2.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    40 loss=99.4323 comp=9.942746 auac_proxy=0.0629 gamma_mean=0.4829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  30%|███       | 60/200 [00:24<00:50,  2.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    60 loss=7.4786 comp=0.747378 auac_proxy=0.0221 gamma_mean=0.4843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  40%|████      | 80/200 [00:32<00:45,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    80 loss=34.9519 comp=3.494709 auac_proxy=-0.0306 gamma_mean=0.4858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  50%|█████     | 100/200 [00:39<00:36,  2.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step   100 loss=349.9907 comp=34.998581 auac_proxy=0.0684 gamma_mean=0.4869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  60%|██████    | 120/200 [00:48<00:30,  2.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step   120 loss=0.0055 comp=0.000063 auac_proxy=-0.1066 gamma_mean=0.4880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  70%|███████   | 140/200 [00:56<00:26,  2.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step   140 loss=1491.0774 comp=149.107254 auac_proxy=-0.0767 gamma_mean=0.4886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  80%|████████  | 160/200 [01:03<00:14,  2.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step   160 loss=1136.3169 comp=113.631203 auac_proxy=-0.0935 gamma_mean=0.4891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1:  90%|█████████ | 180/200 [01:12<00:07,  2.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step   180 loss=108.0205 comp=10.801558 auac_proxy=-0.0279 gamma_mean=0.4899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 200/200 [01:19<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step   200 loss=41486.1445 comp=4148.614258 auac_proxy=-0.0262 gamma_mean=0.4909\n",
            "Epoch 1 completed; steps so far: 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}